{
  "timestamp": "2024-12-12T02:26:14.056250",
  "model_path": "/home/daniel.gebre/Desktop/Thesis/LLM-Compression/experiments/results/final_model",
  "metrics": {
    "accuracy": 0.3048951048951049,
    "latency_ms": 26.574447751045227,
    "throughput": 37.630132876822174,
    "parameter_count": 994115584,
    "active_parameters": 994114250,
    "sparsity": 1.3418962758748876e-06,
    "flops": 383291752448,
    "macs": 191645876224,
    "memory_footprint": {
      "gpu_allocated": 4263.5244140625,
      "gpu_cached": 6774.0,
      "cpu_memory": 1647.109375
    },
    "power_watts": 207.75,
    "co2_emissions": 4.916912721852462,
    "cost_per_inference": 49.70571926437661,
    "compute_metrics": {
      "flops": 383291752448,
      "macs": 191645876224,
      "parameter_count": 994115584,
      "active_parameter_count": 994114250,
      "sparsity": 1.3418962758748876e-06,
      "bandwidth_usage": 9.765625,
      "cache_hits": 1.0,
      "activation_memory": 4407.5322265625,
      "attention_params": 167772160,
      "attention_nonzero": 167771911,
      "attention_sparsity": 1.484155654929431e-06,
      "mlp_params": 563675136,
      "mlp_nonzero": 563674360,
      "mlp_sparsity": 1.3766794921821202e-06,
      "embedding_params": 262668288
    },
    "cost_metrics": {
      "inference_cost_usd": 49.70571926437661,
      "gpu_time_cost": 3.690895520978504e-06,
      "memory_cost": 3.0734810866105046e-06,
      "total_operation_cost": 49.705712500000004
    }
  }
}