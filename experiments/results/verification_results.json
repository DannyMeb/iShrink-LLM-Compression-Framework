{
  "timestamp": "2024-12-10T19:50:06.069924",
  "model_path": "/home/daniel.gebre/Desktop/Thesis/LLM-Compression/experiments/results/final_model",
  "metrics": {
    "accuracy": 0.23426573426573427,
    "latency_ms": 16.85290038585663,
    "throughput": 59.336967353063145,
    "parameter_count": 833093632,
    "active_parameters": 749206526,
    "sparsity": 0.10069349083681389,
    "flops": 253536921600,
    "macs": 126768460800,
    "memory_footprint": {
      "gpu_allocated": 1438.26220703125,
      "gpu_cached": 3858.0,
      "cpu_memory": 1304.3984375
    },
    "power_watts": 158.26,
    "co2_emissions": 4.936025234852897,
    "cost_per_inference": 37.46032929820258,
    "compute_metrics": {
      "flops": 253536921600,
      "macs": 126768460800,
      "parameter_count": 833093632,
      "active_parameter_count": 749206526,
      "sparsity": 0.10069349083681389,
      "bandwidth_usage": -3.90625,
      "cache_hits": 1.0,
      "activation_memory": 1574.27001953125,
      "attention_params": 167772160,
      "attention_nonzero": 83885942,
      "attention_sparsity": 0.500000822544098,
      "mlp_params": 402653184,
      "mlp_nonzero": 402652605,
      "mlp_sparsity": 1.4379620552062988e-06,
      "embedding_params": 262668288
    },
    "cost_metrics": {
      "inference_cost_usd": 37.46032929820258,
      "gpu_time_cost": 2.340680609146754e-06,
      "memory_cost": 6.575219646223948e-07,
      "total_operation_cost": 37.460326300000006
    }
  }
}