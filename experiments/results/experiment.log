=== Experiment Start: Wed Nov 20 09:37:39 PM +04 2024 ===
GPU Information:
Wed Nov 20 21:37:39 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Quadro RTX 6000                On  | 00000000:73:00.0  On |                  Off |
| 33%   34C    P2              67W / 260W |    609MiB / 24576MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A    572256      C   ...ng/.conda/envs/llama-env/bin/python      164MiB |
|    0   N/A  N/A   2658149      G   /usr/lib/xorg/Xorg                          327MiB |
|    0   N/A  N/A   2712508      G   xfwm4                                         4MiB |
|    0   N/A  N/A   4187477      G   ...sion,SpareRendererForSitePerProcess      109MiB |
+---------------------------------------------------------------------------------------+
=== Running Pipeline ===
INFO:__main__:Initialized pruning pipeline with device: cuda
INFO:__main__:Loading model and data...
INFO:src.model_loader:Model not found locally. Downloading...
INFO:src.model_loader:Downloading model meta-llama/Llama-3.2-1B-Instruct...
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.
INFO:src.model_loader:Saving model to models/llama-3.2-1b"
/home/daniel.gebre/.conda/envs/shrinker/lib/python3.10/site-packages/transformers/modeling_utils.py:2869: UserWarning: Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)
  warnings.warn(
Saving checkpoint shards:   0%|          | 0/1 [00:00<?, ?it/s]Saving checkpoint shards:   0%|          | 0/1 [00:01<?, ?it/s]
ERROR:src.model_loader:Failed to load model: Failed to download model: CUDA out of memory. Tried to allocate 502.00 MiB. GPU 0 has a total capacity of 23.62 GiB of which 326.75 MiB is free. Process 572256 has 21.42 GiB memory in use. Including non-PyTorch memory, this process has 1.44 GiB memory in use. Of the allocated memory 1.28 GiB is allocated by PyTorch, and 1.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ERROR:__main__:Pipeline failed: Failed to download model: CUDA out of memory. Tried to allocate 502.00 MiB. GPU 0 has a total capacity of 23.62 GiB of which 326.75 MiB is free. Process 572256 has 21.42 GiB memory in use. Including non-PyTorch memory, this process has 1.44 GiB memory in use. Of the allocated memory 1.28 GiB is allocated by PyTorch, and 1.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/daniel.gebre/Desktop/Thesis/LLM-Compression/src/model_loader.py", line 120, in _download_and_save
    model.save_pretrained(self.local_path)
  File "/home/daniel.gebre/.conda/envs/shrinker/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3019, in save_pretrained
    shard_state_dict = get_state_dict_from_offload(module, module_name, shard_state_dict)
  File "/home/daniel.gebre/.conda/envs/shrinker/lib/python3.10/site-packages/accelerate/utils/modeling.py", line 1577, in get_state_dict_from_offload
    with align_module_device(module, device_to_put_offload):
  File "/home/daniel.gebre/.conda/envs/shrinker/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/daniel.gebre/.conda/envs/shrinker/lib/python3.10/site-packages/accelerate/utils/modeling.py", line 1933, in align_module_device
    set_module_tensor_to_device(module, name, device)
  File "/home/daniel.gebre/.conda/envs/shrinker/lib/python3.10/site-packages/accelerate/utils/modeling.py", line 321, in set_module_tensor_to_device
    new_value = old_value.to(device)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 502.00 MiB. GPU 0 has a total capacity of 23.62 GiB of which 326.75 MiB is free. Process 572256 has 21.42 GiB memory in use. Including non-PyTorch memory, this process has 1.44 GiB memory in use. Of the allocated memory 1.28 GiB is allocated by PyTorch, and 1.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/daniel.gebre/Desktop/Thesis/LLM-Compression/run_pipeline.py", line 72, in run
    model, tokenizer, eval_dataloader = self._setup_model_and_data()
  File "/home/daniel.gebre/Desktop/Thesis/LLM-Compression/run_pipeline.py", line 112, in _setup_model_and_data
    model, tokenizer = model_loader.load()
  File "/home/daniel.gebre/Desktop/Thesis/LLM-Compression/src/model_loader.py", line 41, in load
    return self._download_and_save()
  File "/home/daniel.gebre/Desktop/Thesis/LLM-Compression/src/model_loader.py", line 127, in _download_and_save
    raise RuntimeError(f"Failed to download model: {e}")
RuntimeError: Failed to download model: CUDA out of memory. Tried to allocate 502.00 MiB. GPU 0 has a total capacity of 23.62 GiB of which 326.75 MiB is free. Process 572256 has 21.42 GiB memory in use. Including non-PyTorch memory, this process has 1.44 GiB memory in use. Of the allocated memory 1.28 GiB is allocated by PyTorch, and 1.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/daniel.gebre/Desktop/Thesis/LLM-Compression/src/model_loader.py", line 120, in _download_and_save
    model.save_pretrained(self.local_path)
  File "/home/daniel.gebre/.conda/envs/shrinker/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3019, in save_pretrained
    shard_state_dict = get_state_dict_from_offload(module, module_name, shard_state_dict)
  File "/home/daniel.gebre/.conda/envs/shrinker/lib/python3.10/site-packages/accelerate/utils/modeling.py", line 1577, in get_state_dict_from_offload
    with align_module_device(module, device_to_put_offload):
  File "/home/daniel.gebre/.conda/envs/shrinker/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/daniel.gebre/.conda/envs/shrinker/lib/python3.10/site-packages/accelerate/utils/modeling.py", line 1933, in align_module_device
    set_module_tensor_to_device(module, name, device)
  File "/home/daniel.gebre/.conda/envs/shrinker/lib/python3.10/site-packages/accelerate/utils/modeling.py", line 321, in set_module_tensor_to_device
    new_value = old_value.to(device)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 502.00 MiB. GPU 0 has a total capacity of 23.62 GiB of which 326.75 MiB is free. Process 572256 has 21.42 GiB memory in use. Including non-PyTorch memory, this process has 1.44 GiB memory in use. Of the allocated memory 1.28 GiB is allocated by PyTorch, and 1.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/daniel.gebre/Desktop/Thesis/LLM-Compression/run_pipeline.py", line 315, in <module>
    main()
  File "/home/daniel.gebre/Desktop/Thesis/LLM-Compression/run_pipeline.py", line 312, in main
    pipeline.run()
  File "/home/daniel.gebre/Desktop/Thesis/LLM-Compression/run_pipeline.py", line 72, in run
    model, tokenizer, eval_dataloader = self._setup_model_and_data()
  File "/home/daniel.gebre/Desktop/Thesis/LLM-Compression/run_pipeline.py", line 112, in _setup_model_and_data
    model, tokenizer = model_loader.load()
  File "/home/daniel.gebre/Desktop/Thesis/LLM-Compression/src/model_loader.py", line 41, in load
    return self._download_and_save()
  File "/home/daniel.gebre/Desktop/Thesis/LLM-Compression/src/model_loader.py", line 127, in _download_and_save
    raise RuntimeError(f"Failed to download model: {e}")
RuntimeError: Failed to download model: CUDA out of memory. Tried to allocate 502.00 MiB. GPU 0 has a total capacity of 23.62 GiB of which 326.75 MiB is free. Process 572256 has 21.42 GiB memory in use. Including non-PyTorch memory, this process has 1.44 GiB memory in use. Of the allocated memory 1.28 GiB is allocated by PyTorch, and 1.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
