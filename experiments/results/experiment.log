=== Experiment Start: Mon Nov 25 08:03:56 PM UTC 2024 ===
GPU Information:
Mon Nov 25 20:03:56 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |
| N/A   31C    P0              46W / 400W |      2MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
=== Running Pipeline ===
INFO:__main__:Initialized pruning pipeline with device: cuda
INFO:__main__:Loading model and data...
INFO:src.model_loader:Model not found locally. Downloading...
INFO:src.model_loader:Downloading model meta-llama/Llama-3.2-1B-Instruct...
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
INFO:src.model_loader:Saving model to models/Llama-3.2-1B-Instruct
INFO:src.model_loader:Model saved successfully
INFO:__main__:Evaluating initial model...
INFO:__main__:Creating pruning units...
INFO:src.dependency_graph:Initialized head-level pruning units builder
INFO:src.dependency_graph:Found 16 transformer layers
INFO:src.dependency_graph:Processing last 20% of layers: 14 to 15
INFO:src.dependency_graph:Created 24 head pruning units
INFO:__main__:Loading existing importance scores from experiments/results/importance_scores/head_importance_scores.json
ERROR:__main__:Pipeline failed: PruningPipeline._load_importance_scores() takes 2 positional arguments but 3 were given
Traceback (most recent call last):
  File "/content/LLM-Compression/run_pipeline.py", line 411, in <module>
    main()
  File "/content/LLM-Compression/run_pipeline.py", line 408, in main
    pipeline.run()
  File "/content/LLM-Compression/run_pipeline.py", line 100, in run
    self._load_importance_scores(pruning_units, scores_path)
TypeError: PruningPipeline._load_importance_scores() takes 2 positional arguments but 3 were given
