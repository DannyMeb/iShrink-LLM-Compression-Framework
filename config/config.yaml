# config.yaml

# Model Configuration
model:
  name: "meta-llama/Llama-3.2-1B-Instruct"
  local_path: "models/Llama-3.2-1B-Instruct" 
  device: "cuda"
  precision: "float16"
  batch_size: 8
  max_seq_length: 512
  low_cpu_mem_usage: true
  gradient_checkpointing: true
  hidden_size: 768
  num_heads: 12
  mlp_ratio: 4
  tokenizer_kwargs:
    padding: true
    truncation: true
    return_tensors: "pt"

# Pruning Configuration
pruning:
  targets:
    min_accuracy: 0.40  # 90% of baseline 49.3% MMLU accuracy
    min_accuracy_ratio: 0.90
    compression_target: 0.9
    units_per_step: 10
    max_pruned_heads: 100
    compression_target: 0.98
    min_accuracy: 0.40
    attention_sparsity: 0.0  
    mlp_sparsity: 0.3      
    max_pruned_heads: 100
  
  # Dependency configuration
  dependency:
    hidden_size: 768
    num_heads: 12
    mlp_ratio: 4
    mlp_group_size: 1  # Size of MLP neuron groups
    layer_percentage: 100
  
  # Importance scoring configuration
  importance:
    # Scoring method selection
    scoring_method: "taylor"  # Options: "taylor", "mse", "gradient", "combined"
    weights:
        mse: 0.2
        gradient: 0.2
        taylor: 0.6
    
    # Data sampling - reduced samples for initial testing
    calibration_percent: 1.0  # Reduced from 0.2
    num_samples: 50          # Reduced from 100
    
    # Performance optimization - more conservative settings
    batch_size_per_gpu: 2    # Reduced from 4
    use_mixed_precision: true
    memory_efficient: true
    gradient_accumulation_steps: 8  # Increased from 4
    chunk_size: 5            # Reduced from 10
    clear_cache: true
    
    # Added memory optimization flags
    optimize_memory_usage: true
    empty_cache_between_chunks: true
    
    # Logging and validation
    log_detailed_scores: true
    validation:
        min_non_zero_scores: 0.01
        score_range_check: true
        numerical_stability_check: true
    
    # Fallback options
    fallback_method: "taylor"
    allow_method_switching: true
    
    # Memory management - added safeguards
    max_memory_per_gpu: 19000  # MB
    min_free_memory: 2000      # MB
    emergency_memory_threshold: 0.95


  # Environment configuration
  env:
    max_steps: 1000
    eval_frequency: 5
    eval_batches: 10
    clear_cache: true
    
    
    max_prune_per_step: 5  # Maximum units to prune per step
    reward_weights:
      accuracy: 1.0    # Weight for accuracy maintenance
      compression: 0.5 # Weight for compression progress
      balance: 0.3     # Weight for layer balance
      violation_penalty: 2.0  # Penalty for violating constraints
      
    # Early stopping
    early_stopping:
      patience: 10
      min_delta: 0.01

# RL Configuration
rl:
  ppo:
    hidden_dims: [512, 256]  # Larger network
    learning_rate: 1.0e-4    # Reduced learning rate
    n_epochs: 1             # Reduced epochs
    batch_size: 32
    clip_ratio: 0.2
    value_coef: 0.5
    entropy_coef: 0.01
    max_grad_norm: 0.5
    gamma: 0.99
    gae_lambda: 0.95
    update_interval: 2048   # Increased update interval

# Training Configuration
training:
  data:
    dataset: "cais/mmlu"  # Updated to MMLU
    dataset_config: "all"
    split: "validation"   # Using validation split for evaluation
    batch_size: 8
    max_seq_length: 512
    eval_batch_size: 4
    prefetch_factor: 2
    eval_split: 0.5
    num_workers: 2
    shuffle: false       
  
  optimization:
    num_episodes: 100
    checkpoint_freq: 10
    gradient_accumulation_steps: 4
    mixed_precision: true
    max_gradient_norm: 1.0
    early_stopping:
      patience: 5
      min_delta: 0.01
    memory_efficient_backprop: true
    offload_optimizer: true
    dynamic_batch_sizing: true
  
  logging:
    log_freq: 1
    use_wandb: false
    project_name: "llm-pruning-mmlu"
    metrics_dir: "experiments/results/metrics"
    log_memory_usage: true
    log_dependency_graph: true
    log_importance_scores: true

# Metrics Configuration
metrics:
  eval:
    num_batches: 50
    compute_perplexity: false  # Not needed for MMLU
    measure_latency: true
    measure_throughput: true
    measure_memory: true
  
  thresholds:
    min_accuracy: 0.44        # 90% of baseline 49.3%
    max_latency_increase: 0.2 # Maximum 20% latency increase
    max_memory_usage: 38000   # MB
  
  save:
    dir: "experiments/results/metrics"
    save_initial: true
    save_frequency: 10
    format: "json"

# System Configuration
system:
  seed: 42
  num_workers: 0
  pin_memory: true
  log_level: "INFO"
  save_dir: "experiments/results"
  checkpoint_dir: "experiments/results/checkpoints"
  max_memory_usage: 38000
  memory_monitoring: true
  emergency_memory_recovery: true

# Memory Management
memory:
  gpu_memory_fraction: 0.9
  cleanup_interval: 10
  batch_auto_scaling: true
  min_free_memory: 2000
  monitoring:
    enabled: true
    log_interval: 100
    alert_threshold: 0.95

# Checkpointing
checkpointing:
  save_dir: "experiments/results/checkpoints"
  save_frequency: 10
  keep_last_n: 5
  save_metrics: true
  save_importance_scores: true