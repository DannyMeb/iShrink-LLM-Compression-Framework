# Model Configuration
model:
  name: "meta-llama/Llama-3.2-1B-Instruct"
  local_path: "models/Llama-3.2-1B-Instruct" 
  device: "cuda"
  precision: "float16"
  batch_size: 8
  max_seq_length: 512
  low_cpu_mem_usage: true
  gradient_checkpointing: true
  hidden_size: 768
  num_heads: 12
  mlp_ratio: 4
  tokenizer_kwargs:
    padding: true
    truncation: true
    return_tensors: "pt"

# Pruning Configuration
pruning:
  targets:
    min_accuracy: 0.40  # 90% of baseline 49.3% MMLU accuracy
    min_accuracy_ratio: 0.90
    max_pruned_heads: 100
    compression_target: 0.9
    max_pruned_heads: 100
    compression_target: 0.9
  
  # Dependency configuration
  dependency:
    hidden_size: 768
    num_heads: 12
    mlp_ratio: 4
    mlp_group_size: 128  # Size of MLP neuron groups
  
  # Importance scoring configuration
  importance:
    num_samples: 100  # Increased for MMLU
    batch_size: 8
    use_mixed_precision: true
    memory_efficient: true
    gradient_accumulation: true
    metric_type: 'WIFN'           # FLAP metric: 'WIFN', 'WIFV', or 'IFV'


  # Environment configuration
  env:
    max_steps: 1000
    eval_frequency: 5
    eval_batches: 10
    clear_cache: true
    
    
    max_prune_per_step: 5  # Maximum units to prune per step
    reward_weights:
      accuracy: 1.0    # Weight for accuracy maintenance
      compression: 0.5 # Weight for compression progress
      balance: 0.3     # Weight for layer balance
      violation_penalty: 2.0  # Penalty for violating constraints
      
    # Early stopping
    early_stopping:
      patience: 10
      min_delta: 0.01

# RL Configuration
rl:
  ppo:
    learning_rate: 3.0e-4
    n_epochs: 10
    batch_size: 32
    gamma: 0.99
    gae_lambda: 0.95
    clip_ratio: 0.2
    value_coef: 0.5
    entropy_coef: 0.01
    max_grad_norm: 0.5
    hidden_dims: [256, 256]
    update_interval: 5

# Training Configuration
training:
  data:
    dataset: "cais/mmlu"  # Updated to MMLU
    dataset_config: "all"
    split: "validation"   # Using validation split for evaluation
    batch_size: 8
    max_seq_length: 512
    eval_batch_size: 4
    prefetch_factor: 2
    eval_split: 0.1
    num_workers: 2
    shuffle: false       # No shuffling for evaluation
  
  optimization:
    num_episodes: 100
    checkpoint_freq: 10
    gradient_accumulation_steps: 4
    mixed_precision: true
    max_gradient_norm: 1.0
    early_stopping:
      patience: 5
      min_delta: 0.01
    memory_efficient_backprop: true
    offload_optimizer: true
    dynamic_batch_sizing: true
  
  logging:
    log_freq: 1
    use_wandb: false
    project_name: "llm-pruning-mmlu"
    metrics_dir: "experiments/results/metrics"
    log_memory_usage: true
    log_dependency_graph: true
    log_importance_scores: true

# Metrics Configuration
metrics:
  eval:
    num_batches: 50
    compute_perplexity: false  # Not needed for MMLU
    measure_latency: true
    measure_throughput: true
    measure_memory: true
  
  thresholds:
    min_accuracy: 0.44        # 90% of baseline 49.3%
    max_latency_increase: 0.2 # Maximum 20% latency increase
    max_memory_usage: 38000   # MB
  
  save:
    dir: "experiments/results/metrics"
    save_initial: true
    save_frequency: 10
    format: "json"

# System Configuration
system:
  seed: 42
  num_workers: 0
  pin_memory: true
  log_level: "INFO"
  save_dir: "experiments/results"
  checkpoint_dir: "experiments/results/checkpoints"
  max_memory_usage: 38000
  memory_monitoring: true
  emergency_memory_recovery: true

# Memory Management
memory:
  gpu_memory_fraction: 0.9
  cleanup_interval: 10
  batch_auto_scaling: true
  min_free_memory: 2000
  monitoring:
    enabled: true
    log_interval: 100
    alert_threshold: 0.95

# Checkpointing
checkpointing:
  save_dir: "experiments/results/checkpoints"
  save_frequency: 10
  keep_last_n: 5
  save_metrics: true
  save_importance_scores: true