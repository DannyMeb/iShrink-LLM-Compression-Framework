# Model Configuration
model:
  name: "llama-3.2-1b-instruct"
  local_path: null
  device: "cuda"
  precision: "float16"
  batch_size: 32
  max_seq_length: 512
  tokenizer_kwargs:
    padding: true
    truncation: true
    return_tensors: "pt"

# Pruning Configuration
pruning:
  targets:
    min_accuracy: 0.90
    memory_reduction: 0.5
    latency_reduction: 0.3
  
  dependency:
    min_group_size: 64
    include_skip_connections: true
    optimize_graph: true
    connection_threshold: 0.1
  
  importance:
    methods: ["gradient", "activation", "fisher"]
    weights: [0.4, 0.4, 0.2]
    num_samples: 100
    batch_size: 16
    gradient_accumulation: true

# RL Configuration
rl:
  ppo:
    learning_rate: 3.0e-4
    n_epochs: 10
    batch_size: 64
    gamma: 0.99
    gae_lambda: 0.95
    clip_ratio: 0.2
    value_coef: 0.5
    entropy_coef: 0.01
    max_grad_norm: 0.5
    hidden_dims: [256, 256]
  
  env:
    max_steps: 1000
    warmup_steps: 5
    reward_weights:
      accuracy: 0.6
      memory: 0.2
      latency: 0.2

# Training Configuration
training:
  data:
    eval_split: 0.1
    num_workers: 4
    shuffle: true
  
  optimization:
    num_episodes: 100
    checkpoint_freq: 10
    early_stopping:
      patience: 5
      min_delta: 0.01
  
  logging:
    log_freq: 1
    use_wandb: false
    project_name: "llm-pruning"
    metrics_dir: "experiments/results/metrics"

# System
system:
  seed: 42
  num_workers: 4
  pin_memory: true
  log_level: "INFO"
  save_dir: "experiments/results"
  checkpoint_dir: "experiments/results/checkpoints"