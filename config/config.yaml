# Model Configuration
model:
  name: "meta-llama/Llama-3.2-1B-Instruct"
  local_path: "models/Llama-3.2-1B-Instruct" 
  device: "cuda"
  precision: "float16"
  batch_size: 8
  max_seq_length: 512
  low_cpu_mem_usage: true
  gradient_checkpointing: true
  hidden_size: 768
  num_heads: 12
  mlp_ratio: 4
  tokenizer_kwargs:
    padding: true
    truncation: true
    return_tensors: "pt"

# Pruning Configuration
pruning:
  targets:
    min_accuracy: 0.90
    min_accuracy_ratio: 0.90  # Minimum acceptable accuracy compared to initial
    max_pruned_heads: 100     # Maximum number of heads that can be pruned
    compression_target: 0.5   # Target compression ratio
  
  # Dependency configuration (for attention head identification)
  dependency:
    hidden_size: 768
    num_heads: 12
    mlp_ratio: 4
  
  # Importance scoring configuration
  importance:
    num_samples: 10
    batch_size: 8
    use_mixed_precision: true
    memory_efficient: true
    gradient_accumulation: true

  # Environment configuration
  env:
    max_steps: 1000
    eval_frequency: 5        # How often to evaluate model
    eval_batches: 10        # Number of batches for quick evaluation
    clear_cache: true       # Clear CUDA cache after each step
    
    # Reward weights
    reward_weights:
      accuracy: 1.0         # Weight for accuracy maintenance
      compression: 0.5      # Weight for compression achievement
      importance: 0.3       # Weight for importance-based penalty
    
    # Early stopping
    early_stopping:
      patience: 10
      min_delta: 0.01

# RL Configuration
rl:
  ppo:
    learning_rate: 3.0e-4
    n_epochs: 10
    batch_size: 32
    gamma: 0.99
    gae_lambda: 0.95
    clip_ratio: 0.2
    value_coef: 0.5
    entropy_coef: 0.01
    max_grad_norm: 0.5
    hidden_dims: [256, 256]
    update_interval: 5      # How often to update policy

# Training Configuration
training:
  data:
    dataset: "wikitext"
    dataset_config: "wikitext-2-raw-v1"
    num_samples: 1000
    batch_size: 8
    max_seq_length: 512
    eval_batch_size: 4
    prefetch_factor: 2
    eval_split: 0.1
    num_workers: 2
    shuffle: true
  
  optimization:
    num_episodes: 100
    checkpoint_freq: 10
    gradient_accumulation_steps: 4
    mixed_precision: true
    max_gradient_norm: 1.0
    early_stopping:
      patience: 5
      min_delta: 0.01
    memory_efficient_backprop: true
    offload_optimizer: true
    dynamic_batch_sizing: true
  
  logging:
    log_freq: 1
    use_wandb: false
    project_name: "llm-pruning"
    metrics_dir: "experiments/results/metrics"
    log_memory_usage: true
    log_dependency_graph: true
    log_importance_scores: true

# Metrics Configuration
metrics:
  eval:
    num_batches: 50        # Number of batches for full evaluation
    compute_perplexity: true
    measure_latency: true
    measure_throughput: true
    measure_memory: true
  
  thresholds:
    min_accuracy: 0.90
    max_perplexity: 10.0
    max_latency_increase: 0.2  # Maximum 20% latency increase
    max_memory_usage: 38000    # MB
  
  save:
    dir: "experiments/results/metrics"
    save_initial: true     # Save initial model metrics
    save_frequency: 10     # Save metrics every N steps
    format: "json"         # Format for saving metrics

# System Configuration
system:
  seed: 42
  num_workers: 0
  pin_memory: true
  log_level: "INFO"
  save_dir: "experiments/results"
  checkpoint_dir: "experiments/results/checkpoints"
  max_memory_usage: 38000
  memory_monitoring: true
  emergency_memory_recovery: true

# Memory Management
memory:
  gpu_memory_fraction: 0.9
  cleanup_interval: 10
  batch_auto_scaling: true
  min_free_memory: 2000
  monitoring:
    enabled: true
    log_interval: 100
    alert_threshold: 0.95

# Checkpointing
checkpointing:
  save_dir: "experiments/results/checkpoints"
  save_frequency: 10           # Save every N episodes
  keep_last_n: 5              # Keep only last N checkpoints
  save_metrics: true          # Save metrics with checkpoints
  save_importance_scores: true # Save importance scores with checkpoints